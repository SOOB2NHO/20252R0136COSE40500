#!/usr/bin/env python3
"""
Latex í¬ë§·ì„ ì‚¬ìš©í•˜ì—¬ ì»¬ëŸ¼ ë¶€ë¶„ì§‘í•©ì˜ ì¿¼ë¦¬-í…Œì´ë¸” ìœ ì‚¬ë„ ë¶„ì„

- ëª¨ë“  ê°€ëŠ¥í•œ ì»¬ëŸ¼ ë¶€ë¶„ì§‘í•©(2^n)ì„ ìƒì„±
- ê° ë¶€ë¶„ì§‘í•©ì— ëŒ€í•´ ì¿¼ë¦¬-í…Œì´ë¸” ìœ ì‚¬ë„ ê³„ì‚°
- ìµœê³  ìœ ì‚¬ë„ ë¶€ë¶„ì§‘í•©(best_subset) ì‹ë³„
"""
from __future__ import annotations

import argparse
import json
import re
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence

import matplotlib.pyplot as plt
import numpy as np
from sentence_transformers import SentenceTransformer

# Unbuffered output for real-time logging
sys.stdout.reconfigure(line_buffering=True)
ROOT_DIR = Path(__file__).resolve().parents[2]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))


def latex_table_str(table_array: Sequence[Sequence]) -> str:
    """í…Œì´ë¸” ë°°ì—´ì„ LaTeX tabular í˜•ì‹ìœ¼ë¡œ ë³€í™˜."""
    if not table_array:
        return ""
    
    headers = table_array[0]
    data_rows = table_array[1:]
    
    if not headers:
        return ""
    
    num_cols = len(headers)
    
    # LaTeX íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
    def escape_latex(text: str) -> str:
        if text is None:
            return ""
        text = str(text)
        # LaTeX íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
        text = text.replace("\\", "\\textbackslash{}")
        text = text.replace("{", "\\{")
        text = text.replace("}", "\\}")
        text = text.replace("$", "\\$")
        text = text.replace("&", "\\&")
        text = text.replace("%", "\\%")
        text = text.replace("#", "\\#")
        text = text.replace("^", "\\textasciicircum{}")
        text = text.replace("_", "\\_")
        text = text.replace("~", "\\textasciitilde{}")
        return text
    
    # í—¤ë” ë³€í™˜
    header_strs = [escape_latex(h) if h is not None else "" for h in headers]
    
    # LaTeX tabular ì‹œì‘
    latex = "\\begin{tabular}{|" + "c|" * num_cols + "}\n"
    latex += "\\hline\n"
    
    # í—¤ë” í–‰
    latex += " & ".join(header_strs) + " \\\\\n"
    latex += "\\hline\n"
    
    # ë°ì´í„° í–‰ ë³€í™˜
    for row in data_rows:
        row_strs = [escape_latex(item) if item is not None else "" for item in row]
        # í–‰ ê¸¸ì´ê°€ ì»¬ëŸ¼ ìˆ˜ë³´ë‹¤ ì ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
        while len(row_strs) < num_cols:
            row_strs.append("")
        latex += " & ".join(row_strs[:num_cols]) + " \\\\\n"
    
    latex += "\\hline\n"
    latex += "\\end{tabular}"
    
    return latex


def compute_subset_similarities(
    question: str,
    table_array: Sequence[Sequence],
    model: SentenceTransformer,
    relevant_columns: Sequence[str] | None = None,
) -> List[Dict]:
    """í…Œì´ë¸”ì˜ ëª¨ë“  ì»¬ëŸ¼ ë¶€ë¶„ì§‘í•©ì— ëŒ€í•´ ì¿¼ë¦¬-í…Œì´ë¸” ìœ ì‚¬ë„ë¥¼ ê³„ì‚° (LaTeX ë²„ì „)."""
    if not table_array or len(table_array) < 2:
        raise ValueError("table_arrayëŠ” ìµœì†Œ í—¤ë”ì™€ í•œ ê°œ ì´ìƒì˜ ë°ì´í„° í–‰ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")

    headers = table_array[0]
    rows = table_array[1:]
    num_cols = len(headers)
    
    # ì»¬ëŸ¼ ìˆ˜ê°€ 20ê°œ ì´ˆê³¼ë©´ ì—ëŸ¬ ë°œìƒ (í˜¸ì¶œë¶€ì—ì„œ ìŠ¤í‚µí•˜ë„ë¡)
    if num_cols > 20:
        raise ValueError(f"ì»¬ëŸ¼ ìˆ˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ ({num_cols}ê°œ). ìµœëŒ€ 20ê°œê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤.")

    query_emb = model.encode(question, normalize_embeddings=True)

    rel_columns_set = set(relevant_columns or [])

    subset_infos: List[Dict] = []
    for bits in range(1 << num_cols):
        selected_cols = [
            headers[idx]
            for idx in range(num_cols)
            if (bits >> idx) & 1
        ]

        selected_rows = [
            [
                row[idx] if idx < len(row) else ""
                for idx in range(num_cols)
                if (bits >> idx) & 1
            ]
            for row in rows
        ]

        table_subarray = [selected_cols] + selected_rows if selected_cols else [[]]
        latex_str = latex_table_str(table_subarray)
        table_emb = model.encode(
            latex_str,
            normalize_embeddings=True,
        )

        similarity = float(np.dot(query_emb, table_emb))
        is_relevant = (
            bool(rel_columns_set)
            and len(selected_cols) == len(rel_columns_set)
            and set(selected_cols) == rel_columns_set
        )

        subset_infos.append(
            {
                "bitmask": bits,
                "columns": selected_cols,
                "size": len(selected_cols),
                "similarity": similarity,
                "is_relevant": is_relevant,
            }
        )

    subset_infos.sort(key=lambda item: item["similarity"], reverse=True)
    return subset_infos


def plot_distributions(
    subset_infos: Sequence[Dict],
    output_dir: Path,
    histogram_name: str = "dev_first_table_similarity_latex.png",
    scatter_name: str = "dev_first_table_similarity_by_size_latex.png",
) -> None:
    """ìœ ì‚¬ë„ ë¶„í¬ ë° ì»¬ëŸ¼ ê°œìˆ˜ ëŒ€ë¹„ ìœ ì‚¬ë„ ì‚°ì ë„ë¥¼ ì €ì¥."""
    output_dir.mkdir(parents=True, exist_ok=True)

    similarities = [entry["similarity"] for entry in subset_infos]
    sizes = [entry["size"] for entry in subset_infos]
    relevant_entries = [entry for entry in subset_infos if entry.get("is_relevant")]

    histogram_path = output_dir / histogram_name
    plt.figure(figsize=(8, 5))
    plt.hist(similarities, bins=10, color="#4e79a7", edgecolor="black", alpha=0.75, label="All subsets")
    if relevant_entries:
        rel_sims = [entry["similarity"] for entry in relevant_entries]
        plt.hist(
            rel_sims,
            bins=10,
            color="#f28e2b",
            edgecolor="black",
            alpha=0.85,
            label="Relevant-only subset",
        )
    plt.title("Similarity Distribution (All vs. Relevant Subset) - LaTeX")
    plt.xlabel("Cosine Similarity (normalized embeddings)")
    plt.ylabel("Count")
    if relevant_entries:
        plt.legend()
    plt.tight_layout()
    plt.savefig(histogram_path)
    plt.close()

    scatter_path = output_dir / scatter_name
    plt.figure(figsize=(8, 5))
    scatter = plt.scatter(sizes, similarities, c=similarities, cmap="viridis", s=60, label="All subsets")
    plt.colorbar(scatter, label="Similarity")
    if relevant_entries:
        plt.scatter(
            [entry["size"] for entry in relevant_entries],
            [entry["similarity"] for entry in relevant_entries],
            color="#d62728",
            s=120,
            edgecolors="black",
            label="Relevant-only subset",
        )
    if sizes:
        plt.xticks(range(0, max(sizes) + 1))
    plt.title("Similarity vs. Number of Columns Included (LaTeX)")
    plt.xlabel("Subset Size (# of columns)")
    plt.ylabel("Cosine Similarity")
    plt.grid(alpha=0.3)
    if relevant_entries:
        plt.legend()
    plt.tight_layout()
    plt.savefig(scatter_path)
    plt.close()

    print(f"ğŸ“Š ì €ì¥ ì™„ë£Œ: {histogram_path}")
    print(f"ğŸ“Š ì €ì¥ ì™„ë£Œ: {scatter_path}")


def parse_log_file(log_path: Path) -> List[Dict]:
    """ë¡œê·¸ íŒŒì¼ì—ì„œ ê¸°ì¡´ ê³„ì‚° ê²°ê³¼ë¥¼ íŒŒì‹±.
    
    Returns:
        ê¸°ì¡´ table_results í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸ (subsetsëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸, best/relevant subsetë§Œ í¬í•¨)
    """
    if not log_path.exists():
        return []
    
    results: List[Dict] = []
    current_table: Optional[Dict] = None
    
    with log_path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            
            # Table ì‹œì‘
            table_match = re.match(r"ğŸ“„ Table (\d+) \(feta_id: (.+)\)", line)
            if table_match:
                if current_table:
                    results.append(current_table)
                
                table_num = int(table_match.group(1))
                feta_id = table_match.group(2)
                current_table = {
                    "table_num": table_num,
                    "feta_id": feta_id,
                    "record": {"feta_id": int(feta_id) if feta_id.isdigit() else None},
                    "subsets": [],  # ë¡œê·¸ì—ëŠ” ëª¨ë“  ë¶€ë¶„ì§‘í•© ì •ë³´ê°€ ì—†ìŒ
                    "best_subset": None,
                    "relevant_subset": None,
                }
                continue
            
            if not current_table:
                continue
            
            # ìµœê³  ìœ ì‚¬ë„ íŒŒí‹°ì…˜
            best_match = re.search(
                r"ğŸ” ìµœê³  ìœ ì‚¬ë„ íŒŒí‹°ì…˜: sim=([\d.]+) \(size=(\d+)\) \| cols=\[(.+?)\]",
                line
            )
            if best_match:
                sim = float(best_match.group(1))
                size = int(best_match.group(2))
                cols_str = best_match.group(3)
                # ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
                if cols_str == "<empty>":
                    cols = []
                else:
                    # ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ê³  ë”°ì˜´í‘œ ì œê±°
                    cols = [c.strip().strip("'\"") for c in cols_str.split(",") if c.strip()]
                current_table["best_subset"] = {
                    "similarity": sim,
                    "size": size,
                    "columns": cols,
                }
                continue
            
            # relevant columns íŒŒí‹°ì…˜
            rel_match = re.search(
                r"ğŸ¯ relevant columns íŒŒí‹°ì…˜: sim=([\d.]+) \(size=(\d+)\) \| cols=\[(.+?)\]",
                line
            )
            if rel_match:
                sim = float(rel_match.group(1))
                size = int(rel_match.group(2))
                cols_str = rel_match.group(3)
                if cols_str == "<empty>":
                    cols = []
                else:
                    cols = [c.strip().strip("'\"") for c in cols_str.split(",") if c.strip()]
                current_table["relevant_subset"] = {
                    "similarity": sim,
                    "size": size,
                    "columns": cols,
                }
                continue
    
    # ë§ˆì§€ë§‰ í…Œì´ë¸” ì¶”ê°€
    if current_table:
        results.append(current_table)
    
    return results


def load_records(jsonl_path: Path, limit: int, start_from: int = 1) -> List[Dict]:
    """JSONL íŒŒì¼ì—ì„œ ë ˆì½”ë“œë¥¼ ë¡œë“œ.
    
    Args:
        jsonl_path: JSONL íŒŒì¼ ê²½ë¡œ
        limit: ë¡œë“œí•  ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ (0ì´ë©´ ì „ì²´)
        start_from: ì‹œì‘í•  ë ˆì½”ë“œ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘, ì´ì „ ë ˆì½”ë“œëŠ” ìŠ¤í‚µ)
    """
    records: List[Dict] = []
    skipped = 0
    with jsonl_path.open("r", encoding="utf-8") as file:
        for line_num, line in enumerate(file, 1):
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
            except json.JSONDecodeError as exc:
                print(f"âš ï¸ JSON decode error (line {line_num}): {exc}")
                continue

            input_block = data.get("input") or {}
            question = input_block.get("question")
            table_array = input_block.get("table_array")
            if not question or not table_array:
                continue

            # start_from ì´ì „ ë ˆì½”ë“œëŠ” ìŠ¤í‚µ
            skipped += 1
            if skipped < start_from:
                continue

            # limitì´ ì„¤ì •ë˜ì–´ ìˆê³  ì´ë¯¸ ì¶©ë¶„í•œ ë ˆì½”ë“œë¥¼ ë¡œë“œí–ˆìœ¼ë©´ ì¤‘ë‹¨
            if limit and len(records) >= limit:
                break

            relevant_columns = (
                data.get("output", {}).get("relevant_columns")
                or data.get("output", {}).get("relevant_columns_flat")
            )

            records.append(
                {
                    "question": question,
                    "table_array": table_array,
                    "relevant_columns": relevant_columns,
                    "feta_id": data.get("feta_id"),
                    "instance_id": data.get("instance_id"),
                    "raw": data,
                }
            )
    
    if start_from > 1:
        print(f"â­ï¸  ì²˜ìŒ {start_from - 1}ê°œ ë ˆì½”ë“œë¥¼ ìŠ¤í‚µí–ˆìŠµë‹ˆë‹¤.")
        sys.stdout.flush()
    
    return records


def summarize(subset_infos: Sequence[Dict], top_k: int = 5) -> None:
    """ìƒÂ·í•˜ìœ„ kê°œì˜ ë¶€ë¶„ì§‘í•© ìš”ì•½ ì¶œë ¥."""
    print("\nğŸ” Top subsets by similarity:")
    sys.stdout.flush()
    for entry in list(subset_infos)[:top_k]:
        cols = entry["columns"] if entry["columns"] else ["<empty>"]
        rel_flag = " *relevant*" if entry.get("is_relevant") else ""
        print(f"  size={entry['size']:>2} | sim={entry['similarity']:.4f} | cols={cols}{rel_flag}")
        sys.stdout.flush()

    print("\nğŸ”» Bottom subsets by similarity:")
    sys.stdout.flush()
    for entry in list(subset_infos)[-top_k:]:
        cols = entry["columns"] if entry["columns"] else ["<empty>"]
        rel_flag = " *relevant*" if entry.get("is_relevant") else ""
        print(f"  size={entry['size']:>2} | sim={entry['similarity']:.4f} | cols={cols}{rel_flag}")
        sys.stdout.flush()

    rel_entries = [entry for entry in subset_infos if entry.get("is_relevant")]
    if rel_entries:
        print("\nğŸ¯ Relevant-only subset statistics:")
        sys.stdout.flush()
        for entry in rel_entries:
            print(f"  similarity={entry['similarity']:.4f} | columns={entry['columns']}")
            sys.stdout.flush()
    else:
        print("\nâš ï¸ Relevant-only subsetì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        sys.stdout.flush()


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Compute query similarity distribution across all column subsets (LaTeX version)."
    )
    parser.add_argument(
        "--input",
        type=Path,
        default=Path("/home/subeen/DaisLab/SACU/data/SACU/SACU_dev.jsonl"),
        help="JSONL íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: data/SACU/SACU_dev.jsonl)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("/home/subeen/DaisLab/SACU/data/SACU"),
        help="ì‹œê°í™” ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸: ë°ì´í„° ë””ë ‰í„°ë¦¬)",
    )
    parser.add_argument(
        "--model",
        type=str,
        default="NovaSearch/stella_en_400M_v5",
        help="SentenceTransformer ëª¨ë¸ (ê¸°ë³¸: NovaSearch/stella_en_400M_v5)",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="ìƒ/í•˜ìœ„ ì¶œë ¥ ê°œìˆ˜ (ê¸°ë³¸: 5)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=0,
        help="ë¶„ì„í•  ìƒìœ„ í…Œì´ë¸” ìˆ˜ (0ì´ë©´ ì „ì²´ í…Œì´ë¸” ì‚¬ìš©)",
    )
    parser.add_argument(
        "--start-from",
        type=int,
        default=1,
        help="ì‹œì‘í•  í…Œì´ë¸” ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘, ì´ë¯¸ ì²˜ë¦¬ëœ í…Œì´ë¸”ì„ ìŠ¤í‚µ)",
    )
    parser.add_argument(
        "--violin-name",
        type=str,
        default="dev_top_tables_similarity_violin_latex.png",
        help="ìƒìœ„ í…Œì´ë¸” ë¶„í¬ ê·¸ë˜í”„ íŒŒì¼ëª…",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    records = load_records(args.input, args.limit, args.start_from)
    if not records:
        raise ValueError("ë¶„ì„í•  ë ˆì½”ë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    model_kwargs: Dict[str, Any] = {}
    if "stella" in args.model.lower():
        model_kwargs["trust_remote_code"] = True
    model = SentenceTransformer(args.model, **model_kwargs)

    table_results: List[Dict] = []
    
    # start_fromì„ ê³ ë ¤í•˜ì—¬ ì¸ë±ìŠ¤ ì¡°ì •
    start_idx = args.start_from
    for idx, record in enumerate(records, start=start_idx):
        print(f"\n{'='*80}")
        sys.stdout.flush()
        feta_id = record.get("feta_id")
        label = feta_id if feta_id is not None else f"index-{idx}"
        print(f"ğŸ“„ Table {idx} (feta_id: {label}) [LaTeX]")
        sys.stdout.flush()
        print(f"ğŸ“ Question: {record['question']}")
        sys.stdout.flush()

        table_array = record["table_array"]
        relevant_columns = record.get("relevant_columns") or []
        headers = table_array[0] if table_array else []
        num_cols = len(headers)
        print(f"ğŸ“‹ Headers ({num_cols}): {headers}")
        sys.stdout.flush()
        
        # ì»¬ëŸ¼ ìˆ˜ê°€ 20ê°œ ì´ˆê³¼ë©´ ìŠ¤í‚µ
        if num_cols > 20:
            print(f"âš ï¸  ì»¬ëŸ¼ ìˆ˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤ ({num_cols}ê°œ > 20ê°œ). ì´ í…Œì´ë¸”ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            sys.stdout.flush()
            continue
        
        if relevant_columns:
            print(f"ğŸ¯ Relevant columns: {relevant_columns}")
            sys.stdout.flush()
        else:
            print("âš ï¸ Relevant columns ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.stdout.flush()

        try:
            subset_infos = compute_subset_similarities(
                record["question"],
                table_array,
                model=model,
                relevant_columns=relevant_columns,
            )
        except ValueError as e:
            print(f"âš ï¸  ì—ëŸ¬ ë°œìƒ: {e}. ì´ í…Œì´ë¸”ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            sys.stdout.flush()
            continue

        best_subset = subset_infos[0] if subset_infos else None
        relevant_subset = next((entry for entry in subset_infos if entry.get("is_relevant")), None)

        if best_subset:
            print(
                f"   ğŸ” ìµœê³  ìœ ì‚¬ë„ íŒŒí‹°ì…˜: sim={best_subset['similarity']:.4f} "
                f"(size={best_subset['size']}) | cols={best_subset['columns'] or ['<empty>']}"
            )
            sys.stdout.flush()
        if relevant_subset:
            print(
                f"   ğŸ¯ relevant columns íŒŒí‹°ì…˜: sim={relevant_subset['similarity']:.4f} "
                f"(size={relevant_subset['size']}) | cols={relevant_subset['columns']}"
            )
            sys.stdout.flush()
        else:
            print("   âš ï¸ Relevant columns ì¡°í•©ê³¼ ì¼ì¹˜í•˜ëŠ” íŒŒí‹°ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            sys.stdout.flush()

        summarize(subset_infos, top_k=args.top_k)

        table_results.append(
            {
                "record": record,
                "subsets": subset_infos,
                "best_subset": best_subset,
                "relevant_subset": relevant_subset,
            }
        )

    # ìƒìœ„ í…Œì´ë¸” ì „ì²´ ë¶„í¬ ê·¸ë˜í”„
    output_dir = args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    # ë¶„í¬ ë°ì´í„° ìƒì„± (subsetsê°€ ìˆëŠ” ê²½ìš°ë§Œ, ë¡œê·¸ì—ì„œ íŒŒì‹±í•œ ê²½ìš°ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    distributions = []
    for result in table_results:
        if result.get("subsets"):
            # ì‹¤ì œ ê³„ì‚°ëœ subsetsê°€ ìˆëŠ” ê²½ìš°
            dist = [entry["similarity"] for entry in result["subsets"]]
        else:
            # ë¡œê·¸ì—ì„œ íŒŒì‹±í•œ ê²½ìš°: bestì™€ relevant subsetë§Œìœ¼ë¡œ ê·¼ì‚¬ ë¶„í¬ ìƒì„±
            # (ì‹¤ì œ ë¶„í¬ëŠ” ì•„ë‹ˆì§€ë§Œ ì  í‘œì‹œëŠ” ê°€ëŠ¥)
            dist = []
            if result.get("best_subset"):
                dist.append(result["best_subset"]["similarity"])
            if result.get("relevant_subset"):
                dist.append(result["relevant_subset"]["similarity"])
            if not dist:
                dist = [0.0]
        distributions.append(dist)
    positions = np.arange(1, len(distributions) + 1)

    plt.figure(figsize=(max(12, len(distributions) * 1.3), 6))
    violin_parts = plt.violinplot(
        distributions,
        positions=positions,
        showmeans=True,
        showextrema=False,
    )
    for body in violin_parts["bodies"]:
        body.set_facecolor("#4e79a7")
        body.set_alpha(0.45)
    if "cmeans" in violin_parts:
        violin_parts["cmeans"].set_edgecolor("#2f4b7c")
        violin_parts["cmeans"].set_linewidth(1.5)

    # ìµœê³  íŒŒí‹°ì…˜ê³¼ relevant íŒŒí‹°ì…˜ ì  í‘œì‹œ
    best_x: List[float] = []
    best_y: List[float] = []
    relevant_x: List[float] = []
    relevant_y: List[float] = []

    for pos, result in zip(positions, table_results):
        best = result["best_subset"]
        if best:
            best_x.append(pos)
            best_y.append(best["similarity"])
        rel = result["relevant_subset"]
        if rel:
            relevant_x.append(pos)
            relevant_y.append(rel["similarity"])

    legend_handles = []
    legend_labels = []
    if best_x:
        best_scatter = plt.scatter(
            best_x,
            best_y,
            color="#1f77b4",
            marker="D",
            s=70,
            label="Best subset",
        )
        legend_handles.append(best_scatter)
        legend_labels.append("Best subset")
    if relevant_x:
        rel_scatter = plt.scatter(
            relevant_x,
            relevant_y,
            color="#d62728",
            s=90,
            edgecolors="black",
            label="Relevant subset",
        )
        legend_handles.append(rel_scatter)
        legend_labels.append("Relevant subset")

    table_labels: List[str] = []
    for result in table_results:
        record = result.get("record", {})
        # ë¡œê·¸ì—ì„œ íŒŒì‹±í•œ ê²½ìš° table_num ì‚¬ìš©
        if "table_num" in result:
            label = str(result.get("feta_id", result.get("table_num", "")))
        else:
            label = record.get("feta_id") or record.get("instance_id") or ""
        table_labels.append(str(label))

    plt.xticks(positions, table_labels, rotation=45, ha="right")
    plt.xlabel("Table (feta_id or index)")
    plt.ylabel("Cosine Similarity")
    plt.title("Top Dev Tables: Similarity Distribution Across Column Subsets (LaTeX)")
    plt.grid(axis="y", alpha=0.3)
    if legend_handles:
        plt.legend(legend_handles, legend_labels, loc="best")

    violin_path = output_dir / args.violin_name
    plt.tight_layout()
    plt.savefig(violin_path)
    plt.close()
    print(f"\nğŸ“Š ìƒìœ„ í…Œì´ë¸” ë¶„í¬ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {violin_path}")
    sys.stdout.flush()


if __name__ == "__main__":
    main()

